1. Python의 request 패키지와 BeautifulSoup을 이용해서
   교보문고 베스트셀러 페이지에서 도서명과 저자, 가격을 
   스크래핑해서 kyobobest.json으로 저장하세요.
   (단, 보이는 페이지만 대상으로 한다)  
    (http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?orderClick=d79)

from bs4 import BeautifulSoup
import requests
import json

headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' }

url = 'http://www.kyobobook.co.kr/bestSellerNew/bestseller.laf?orderClick=d79'

res = requests.get(url, headers=headers)
rss = BeautifulSoup(res.text, 'lxml')

json_datas = []

for kyobo in rss.find_all('title'):
    bkname = kyobo.bkname.text
    writer = kyobo.writer.text
    price = kyobo.price.text

    print(bkname, writer, price)

    json_datas.append( { 'bkname':bkname, 'writer':writer, 'price':price } )

with open('data/kyobobest.json', 'w', encoding='utf-8') as f:
    json.dump(json_datas, f, ensure_ascii=False, indent=2)


2. Python의 request 패키지와 BeautifulSoup을 이용해서
   daum 뉴스에서 2018. 4. 27일자 jtbc 뉴스 기사의 
   제목과 요약을 스크래핑해서 jtbc20180427.csv로 저장하세요
   (단, 보이는 페이지만 대상으로 한다)  
    (https://media.daum.net/cp/310)
	
	from bs4 import BeautifulSoup
import requests
import csv

headers = { 'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36' }

url = 'https://media.daum.net/cp/310?regDate=20180427'

res = requests.get(url, headers=headers)
rss = BeautifulSoup(res.text, 'lxml')

# 수집한 데이터를 저장하기 위한 변수 정의
csv_datas = []

for daum in rss.find_all('data'):
    hdlines = daum.tmef.text
    descnews = daum.wf.text

    print(hdlines, descnews)

    csv_datas.append([hdlines, descnews])

# 저장한 날씨 데이터를 csv로 저장함
with open('data/jtbc20180427.csv', 'w', encoding='utf-8') as f:
    writer = csv.writer(f)
    writer.writerows(csv_datas)
